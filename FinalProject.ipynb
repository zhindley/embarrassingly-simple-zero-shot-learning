{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'AWA2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Path to run on your own system\n",
    "res101 = scipy.io.loadmat('C:/Users/Zach/Documents/NorthEastern/SmallDataMachineLearning/Code/ESZSL/Data/xlsa17/data/'+dataset+'/res101.mat')\n",
    "att_splits = scipy.io.loadmat('C:/Users/Zach/Documents/NorthEastern/SmallDataMachineLearning/Code/ESZSL/Data/xlsa17/data/'+dataset+'/att_splits.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'features', 'image_files', 'labels'])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res101.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'allclasses_names', 'att', 'original_att', 'test_seen_loc', 'test_unseen_loc', 'train_loc', 'trainval_loc', 'val_loc'])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the correct naming conventions to get the loctions\n",
    "trainval_loc = 'trainval_loc'\n",
    "train_loc = 'train_loc'\n",
    "val_loc = 'val_loc'\n",
    "test_loc = 'test_unseen_loc'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the corresponding ground-truth labels/classes for each training example for all our train, val, trainval and test set according to the split locations provided.\n",
    "In this example we have used the `CUB` dataset which has 200 unique classes overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RES101 is Resnet 101, this is the output from CNN\n",
    "labels = res101['labels']\n",
    "#Array of training labels (class label)\n",
    "labels_train = labels[np.squeeze(att_splits[train_loc]-1)]\n",
    "#array of validation labels\n",
    "labels_val = labels[np.squeeze(att_splits[val_loc]-1)]\n",
    "#train + validation labels\n",
    "labels_trainval = labels[np.squeeze(att_splits[trainval_loc]-1)]\n",
    "#test labels\n",
    "labels_test = labels[np.squeeze(att_splits[test_loc]-1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30]\n",
      " [30]\n",
      " [30]\n",
      " [30]\n",
      " [30]\n",
      " [30]\n",
      " [30]\n",
      " [30]\n",
      " [30]\n",
      " [30]]\n"
     ]
    }
   ],
   "source": [
    "print(labels_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43],\n",
       "       [22],\n",
       "       [43],\n",
       "       [38],\n",
       "       [19],\n",
       "       [18],\n",
       "       [46],\n",
       "       [19],\n",
       "       [49],\n",
       "       [37]], dtype=uint8)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking first ten labels\n",
    "labels_train[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list all unique labels, should be 200 for birds\n",
    "unique_labels = np.unique(labels)\n",
    "unique_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical zero-shot learning scenario, there are no overlapping classes between training and testing phase, i.e the train classes are completely different from the test classes. So let us verify if there are any overlapping classes in the test and train scenario.\n",
    "- During training phase we have `z` classes\n",
    "- During the testing phase we have `z'` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_seen = np.unique(labels_train)\n",
    "val_labels_unseen = np.unique(labels_val)\n",
    "trainval_labels_seen = np.unique(labels_trainval)\n",
    "test_labels_unseen = np.unique(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping classes between train and val: 0\n",
      "Number of overlapping classes between trainval and test: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of overlapping classes between train and val:\",len(set(train_labels_seen).intersection(set(val_labels_unseen))))\n",
    "print(\"Number of overlapping classes between trainval and test:\",len(set(trainval_labels_seen).intersection(set(test_labels_unseen))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_train now should be an index into the train_labels seen\n",
    "#example: anything that had 200 as a label, now is 99 which is the last index in train_labels seen\n",
    "#which aligns with label 200\n",
    "i = 0\n",
    "for labels in train_labels_seen:\n",
    "    labels_train[labels_train == labels] = i    \n",
    "    i = i+1\n",
    "j = 0\n",
    "for labels in val_labels_unseen:\n",
    "    labels_val[labels_val == labels] = j\n",
    "    j = j+1\n",
    "k = 0\n",
    "for labels in trainval_labels_seen:\n",
    "    labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "l = 0\n",
    "for labels in test_labels_unseen:\n",
    "    labels_test[labels_test == labels] = l\n",
    "    l = l+1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote the features X ∈ [d×m] available at training stage, where d is the dimensionality\n",
    "of the data, and m is the number of instances. We are useing resnet features which are extracted from `CUB` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all features of all data\n",
    "X_features = res101['features']\n",
    "#select those for the training\n",
    "train_vec = X_features[:,np.squeeze(att_splits[train_loc]-1)]\n",
    "#validation set\n",
    "val_vec = X_features[:,np.squeeze(att_splits[val_loc]-1)]\n",
    "#train+validation\n",
    "trainval_vec = X_features[:,np.squeeze(att_splits[trainval_loc]-1)]\n",
    "#last the test features\n",
    "test_vec = X_features[:,np.squeeze(att_splits[test_loc]-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for train: (2048, 16187)\n",
      "Features for val: (2048, 7340)\n",
      "Features for trainval: (2048, 23527)\n",
      "Features for test: (2048, 7913)\n"
     ]
    }
   ],
   "source": [
    "#looks like resnet101 outputs 2048 features\n",
    "print(\"Features for train:\", train_vec.shape)\n",
    "print(\"Features for val:\", val_vec.shape)\n",
    "print(\"Features for trainval:\", trainval_vec.shape)\n",
    "print(\"Features for test:\", test_vec.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(vec,mean,std):\n",
    "    sol = vec - mean\n",
    "    sol1 = sol/std\n",
    "    return sol1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the classes in the dataset have an attribute (a) description. This vector is known as the `Signature matrix` of dimension S ∈ [0, 1]a×z. For training stage there are z classes and z' classes  for test S ∈ [0, 1]a×z'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attribute Signature matrix (all)\n",
    "#All normalized to have unit 12 norm\n",
    "signature = att_splits['att']\n",
    "#getting the training,validation,train+val,and the test signatures\n",
    "train_sig = signature[:,(train_labels_seen)-1]\n",
    "val_sig = signature[:,(val_labels_unseen)-1]\n",
    "trainval_sig = signature[:,(trainval_labels_seen)-1]\n",
    "test_sig = signature[:,(test_labels_unseen)-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a signature matrix, where the occurance of an attribute corresponding to the class is give.\n",
    "For instance, if the classes are `horse` and `zebra` and the corresponding attributes are [wild_animal, 4_legged, carnivore]\n",
    "\n",
    "```\n",
    " Horse      Zebra\n",
    "[0.00354613 0.        ] Domestic_animal\n",
    "[0.13829921 0.20209503] 4_legged\n",
    "[0.06560347 0.04155225] carnivore\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00375358  0.22753174]\n",
      " [ 0.0463192   0.01150855]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#for the Caltech Birds(CUB) there are 311 attributes\n",
    "print(trainval_sig[3:6,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature for train: (85, 27)\n",
      "Signature for val: (85, 13)\n",
      "Signature for trainval: (85, 40)\n",
      "Signature for test: (85, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Signature for train:\", train_sig.shape)\n",
    "print(\"Signature for val:\", val_sig.shape)\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)\n",
    "print(\"Signature for test:\", test_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for train and val set\n",
    "m_train = labels_train.shape[0]\n",
    "n_val = labels_val.shape[0]\n",
    "z_train = len(train_labels_seen)\n",
    "z1_val = len(val_labels_unseen)\n",
    "\n",
    "#params for trainval and test set\n",
    "m_trainval = labels_trainval.shape[0]\n",
    "n_test = labels_test.shape[0]\n",
    "z_trainval = len(trainval_labels_seen)\n",
    "z1_test = len(test_labels_unseen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth is a one-hot encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting ground truths for the the classes\n",
    "#ground truth for train \n",
    "gt_train = 0*np.ones((m_train, z_train))\n",
    "gt_train[np.arange(m_train), np.squeeze(labels_train)] = 1\n",
    "\n",
    "#grountruth for trainval \n",
    "gt_trainval = 0*np.ones((m_trainval, z_trainval))\n",
    "gt_trainval[np.arange(m_trainval), np.squeeze(labels_trainval)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_train[:1,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07514072 0.22781304 0.09419869 ... 0.14812937 0.89535171 0.26911923]\n",
      " [0.16373933 0.1186293  0.22807321 ... 0.47775361 0.89335    0.5642668 ]\n",
      " [0.44017351 0.38097548 0.07756222 ... 0.32541005 0.32501173 0.93339768]\n",
      " ...\n",
      " [0.115406   0.200567   0.87017679 ... 0.26915815 0.8351356  0.21022228]\n",
      " [0.00708325 0.71062486 0.98077692 ... 0.01685801 0.67188563 0.87969359]\n",
      " [0.53139052 0.92698919 0.6167304  ... 0.0345136  0.04086571 0.59086512]]\n"
     ]
    }
   ],
   "source": [
    "#train set dimensions\n",
    "d_train = train_vec.shape[0] #num of dimensions of features\n",
    "a_train = train_sig.shape[0] #num of attributes\n",
    "\n",
    "#Weights matrix\n",
    "#V = np.random.rand(d_train,a_train)\n",
    "#print(V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add the formula in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import normalize \n",
    "\n",
    "V = torch.tensor(np.random.rand(d_train,a_train) * 1e-5, requires_grad=True)\n",
    "#print(V)\n",
    "\n",
    "'''\n",
    "part_1 = np.linalg.pinv(np.matmul(train_vec, train_vec.transpose()) + (10**1)*np.eye(d_train))\n",
    "part_0 = np.matmul(np.matmul(train_vec,gt_train),train_sig.transpose())\n",
    "part_2 = np.linalg.pinv(np.matmul(train_sig, train_sig.transpose()) + (10**1)*np.eye(a_train))\n",
    "\n",
    "V  = torch.tensor(np.matmul(np.matmul(part_1,part_0),part_2))\n",
    "'''\n",
    "\n",
    "#need my values X,V,S, and Y\n",
    "#V = torch.tensor(V,requires_grad=True)\n",
    "#going to reassign\n",
    "X = torch.tensor(train_vec)\n",
    "S = torch.tensor(train_sig)\n",
    "Y = torch.tensor(gt_train)\n",
    "#print(S)\n",
    "print(Y)\n",
    "\n",
    "def forward(X,V,S):\n",
    "    xT = torch.transpose(X,0,1)\n",
    "    mult1 = torch.matmul(xT,V)\n",
    "    ans = torch.matmul(mult1,S)\n",
    "    #print(ans)\n",
    "    return ans\n",
    "\n",
    "#regularization term\n",
    "def reg (V,X,S,g,l,b):\n",
    "\n",
    "    #Calculate frobenius norm of VS\n",
    "    part1 = g*(torch.norm(torch.matmul(V,S)))\n",
    "    #Calculate the Frobenius norm of XTV\n",
    "    part2 = l * (torch.norm(torch.matmul(torch.transpose(X,0,1),V)))\n",
    "    #Calculate the frobenius norm of V\n",
    "    part3 = b * (torch.norm(V))\n",
    "    return part1 + part2 + part3\n",
    "\n",
    "\n",
    "def loss_func(y_pred,Y):\n",
    "    return torch.norm(torch.subtract(y_pred,Y))\n",
    "\n",
    "\n",
    "#Test values, just getting it to work\n",
    "iter = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    if V.grad is not None:\n",
    "        V.grad.data.zero_()\n",
    "    \n",
    "    y_pred = forward(X,V,S)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    #print(mse_loss(y_pred,Y))\n",
    "    #print(reg(V,X,S,.01,.03,.03))\n",
    "    #+ reg(V,X,S,3,1,3)\n",
    "    loss = loss_func(y_pred,Y) + reg(V,X,S,3,1,3)\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        V -= learning_rate * V.grad\n",
    "\n",
    "    #need to calculate error\n",
    "    mes = nn.MSELoss()\n",
    "    if loss < 0.01:\n",
    "        print('it converged')\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07514072 0.22781304 0.09419869 ... 0.14812937 0.89535171 0.26911923]\n",
      " [0.16373933 0.1186293  0.22807321 ... 0.47775361 0.89335    0.5642668 ]\n",
      " [0.44017351 0.38097548 0.07756222 ... 0.32541005 0.32501173 0.93339768]\n",
      " ...\n",
      " [0.115406   0.200567   0.87017679 ... 0.26915815 0.8351356  0.21022228]\n",
      " [0.00708325 0.71062486 0.98077692 ... 0.01685801 0.67188563 0.87969359]\n",
      " [0.53139052 0.92698919 0.6167304  ... 0.0345136  0.04086571 0.59086512]]\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matmul(): argument 'other' (position 2) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[412], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m sPrime \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(test_sig)\n\u001b[0;32m      7\u001b[0m \u001b[39m#labels_test is indexes to test_labels_seen\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m#yTest = torch.tensor(labels_test)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[39m#calculates the outputs\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m outputs_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(torch\u001b[39m.\u001b[39;49mmatmul(torch\u001b[39m.\u001b[39;49mtranspose(xPrime,\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m),V),sPrime)\n\u001b[0;32m     12\u001b[0m \u001b[39m#calculating the predictions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m preds_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([torch\u001b[39m.\u001b[39margmax(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs_1])\t\t\n",
      "\u001b[1;31mTypeError\u001b[0m: matmul(): argument 'other' (position 2) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "#Inference stage\n",
    "outPut = torch.tensor([])\n",
    "\n",
    "#Label test data\n",
    "xPrime = torch.tensor(test_vec)\n",
    "sPrime = torch.tensor(test_sig)\n",
    "#labels_test is indexes to test_labels_seen\n",
    "#yTest = torch.tensor(labels_test)\n",
    "\n",
    "#calculates the outputs\n",
    "outputs_1 = torch.matmul(torch.matmul(torch.transpose(xPrime,0,1),V),sPrime)\n",
    "#calculating the predictions\n",
    "preds_1 = torch.tensor([torch.argmax(output) for output in outputs_1])\t\t\n",
    "\n",
    "#creating confusion?\n",
    "cm = confusion_matrix(labels_test, preds_1)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "avg = sum(cm.diagonal())/len(test_labels_unseen)\n",
    "\n",
    "total = len(preds_1)\n",
    "correct = 0\n",
    "for i in range(total):      \n",
    "    if preds_1[i] == torch.tensor(labels_test[i]):\n",
    "        correct = correct + 1\n",
    "\n",
    "print(\"The top accuracy is:\", 100 * correct / (1.0 * total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
