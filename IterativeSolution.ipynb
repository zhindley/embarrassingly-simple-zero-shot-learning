{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please add the folder name of the dataset to run it on different dataset.\n",
    "dataset = 'AWA1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "\n",
    "#This will need to be updated to run on your own machine.\n",
    "\n",
    "############################################################################### \n",
    "res101 = scipy.io.loadmat('C:/Users/Zach/Documents/NorthEastern/SmallDataMachineLearning/Code/ESZSL/Data/xlsa17/data/'+dataset+'/res101.mat')\n",
    "att_splits = scipy.io.loadmat('C:/Users/Zach/Documents/NorthEastern/SmallDataMachineLearning/Code/ESZSL/Data/xlsa17/data/'+dataset+'/att_splits.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'image_files', 'features', 'labels'])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res101.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'allclasses_names', 'att', 'original_att', 'test_seen_loc', 'test_unseen_loc', 'train_loc', 'trainval_loc', 'val_loc'])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the correct naming conventions to get the loctions\n",
    "trainval_loc = 'trainval_loc'\n",
    "train_loc = 'train_loc'\n",
    "val_loc = 'val_loc'\n",
    "test_loc = 'test_unseen_loc'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the corresponding ground-truth labels/classes for each training example for all our train, val, trainval and test set according to the split locations provided.\n",
    "In this example we have used the `CUB` dataset which has 200 unique classes overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RES101 is Resnet 101, this is the output from CNN\n",
    "labels = res101['labels']\n",
    "#Array of training labels (class label)\n",
    "labels_train = labels[np.squeeze(att_splits[train_loc]-1)]\n",
    "#array of validation labels\n",
    "labels_val = labels[np.squeeze(att_splits[val_loc]-1)]\n",
    "#train + validation labels\n",
    "labels_trainval = labels[np.squeeze(att_splits[trainval_loc]-1)]\n",
    "#test labels\n",
    "labels_test = labels[np.squeeze(att_splits[test_loc]-1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list all unique labels, should be 200 for birds\n",
    "unique_labels = np.unique(labels)\n",
    "unique_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical zero-shot learning scenario, there are no overlapping classes between training and testing phase, i.e the train classes are completely different from the test classes. So let us verify if there are any overlapping classes in the test and train scenario.\n",
    "- During training phase we have `z` classes\n",
    "- During the testing phase we have `z'` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_seen = np.unique(labels_train)\n",
    "val_labels_unseen = np.unique(labels_val)\n",
    "trainval_labels_seen = np.unique(labels_trainval)\n",
    "test_labels_unseen = np.unique(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping classes between train and val: 0\n",
      "Number of overlapping classes between trainval and test: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of overlapping classes between train and val:\",len(set(train_labels_seen).intersection(set(val_labels_unseen))))\n",
    "print(\"Number of overlapping classes between trainval and test:\",len(set(trainval_labels_seen).intersection(set(test_labels_unseen))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_train now should be an index into the train_labels seen\n",
    "#example: anything that had 200 as a label, now is 99 which is the last index in train_labels seen\n",
    "#which aligns with label 200\n",
    "i = 0\n",
    "for labels in train_labels_seen:\n",
    "    labels_train[labels_train == labels] = i    \n",
    "    i = i+1\n",
    "j = 0\n",
    "for labels in val_labels_unseen:\n",
    "    labels_val[labels_val == labels] = j\n",
    "    j = j+1\n",
    "k = 0\n",
    "for labels in trainval_labels_seen:\n",
    "    labels_trainval[labels_trainval == labels] = k\n",
    "    k = k+1\n",
    "l = 0\n",
    "for labels in test_labels_unseen:\n",
    "    labels_test[labels_test == labels] = l\n",
    "    l = l+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote the features X ∈ [d×m] available at training stage, where d is the dimensionality\n",
    "of the data, and m is the number of instances. We are useing resnet features which are extracted from `CUB` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all features of all data\n",
    "X_features = res101['features']\n",
    "#select those for the training\n",
    "train_vec = X_features[:,np.squeeze(att_splits[train_loc]-1)]\n",
    "#validation set\n",
    "val_vec = X_features[:,np.squeeze(att_splits[val_loc]-1)]\n",
    "#train+validation\n",
    "trainval_vec = X_features[:,np.squeeze(att_splits[trainval_loc]-1)]\n",
    "#last the test features\n",
    "test_vec = X_features[:,np.squeeze(att_splits[test_loc]-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for train: (2048, 13460)\n",
      "Features for val: (2048, 6372)\n",
      "Features for trainval: (2048, 19832)\n",
      "Features for test: (2048, 5685)\n"
     ]
    }
   ],
   "source": [
    "#looks like resnet101 outputs 2048 features\n",
    "print(\"Features for train:\", train_vec.shape)\n",
    "print(\"Features for val:\", val_vec.shape)\n",
    "print(\"Features for trainval:\", trainval_vec.shape)\n",
    "print(\"Features for test:\", test_vec.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(vec,mean,std):\n",
    "    sol = vec - mean\n",
    "    sol1 = sol/std\n",
    "    return sol1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the classes in the dataset have an attribute (a) description. This vector is known as the `Signature matrix` of dimension S ∈ [0, 1]a×z. For training stage there are z classes and z' classes  for test S ∈ [0, 1]a×z'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attribute Signature matrix (all)\n",
    "#All normalized to have unit 12 norm\n",
    "signature = att_splits['att']\n",
    "#getting the training,validation,train+val,and the test signatures\n",
    "train_sig = signature[:,(train_labels_seen)-1]\n",
    "val_sig = signature[:,(val_labels_unseen)-1]\n",
    "trainval_sig = signature[:,(trainval_labels_seen)-1]\n",
    "test_sig = signature[:,(test_labels_unseen)-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a signature matrix, where the occurance of an attribute corresponding to the class is give.\n",
    "For instance, if the classes are `horse` and `zebra` and the corresponding attributes are [wild_animal, 4_legged, carnivore]\n",
    "\n",
    "```\n",
    " Horse      Zebra\n",
    "[0.00354613 0.        ] Domestic_animal\n",
    "[0.13829921 0.20209503] 4_legged\n",
    "[0.06560347 0.04155225] carnivore\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00375358  0.22753174]\n",
      " [ 0.0463192   0.01150855]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#for the Caltech Birds(CUB) there are 311 attributes\n",
    "print(trainval_sig[3:6,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature for train: (85, 27)\n",
      "Signature for val: (85, 13)\n",
      "Signature for trainval: (85, 40)\n",
      "Signature for test: (85, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Signature for train:\", train_sig.shape)\n",
    "print(\"Signature for val:\", val_sig.shape)\n",
    "print(\"Signature for trainval:\", trainval_sig.shape)\n",
    "print(\"Signature for test:\", test_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for train and val set\n",
    "m_train = labels_train.shape[0]\n",
    "n_val = labels_val.shape[0]\n",
    "z_train = len(train_labels_seen)\n",
    "z1_val = len(val_labels_unseen)\n",
    "\n",
    "#params for trainval and test set\n",
    "m_trainval = labels_trainval.shape[0]\n",
    "n_test = labels_test.shape[0]\n",
    "z_trainval = len(trainval_labels_seen)\n",
    "z1_test = len(test_labels_unseen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth is a one-hot encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting ground truths for the the classes\n",
    "#ground truth for train \n",
    "gt_train = 0*np.ones((m_train, z_train))\n",
    "gt_train[np.arange(m_train), np.squeeze(labels_train)] = 1\n",
    "\n",
    "#grountruth for trainval \n",
    "gt_trainval = 0*np.ones((m_trainval, z_trainval))\n",
    "gt_trainval[np.arange(m_trainval), np.squeeze(labels_trainval)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_train[:1,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set dimensions\n",
    "d_train = train_vec.shape[0] #num of dimensions of features\n",
    "a_train = train_sig.shape[0] #num of attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "VIter = torch.tensor(np.random.rand(d_train,a_train)*1e-4, requires_grad=True)\n",
    "\n",
    "#going to reassign\n",
    "X = torch.tensor(train_vec)\n",
    "S = torch.tensor(train_sig)\n",
    "Y = torch.tensor(gt_train)\n",
    "\n",
    "def forward(X,V,S):\n",
    "    xT = torch.transpose(X,0,1)\n",
    "    mult1 = torch.matmul(xT,V)\n",
    "    ans = torch.matmul(mult1,S)\n",
    "    #print(ans)\n",
    "    return ans\n",
    "\n",
    "#lossFunction\n",
    "def loss_func(y_pred,Y):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(y_pred,Y)\n",
    "\n",
    "#regularization term\n",
    "def reg (V,X,S,g,l,b):\n",
    "    #Calculate frobenius norm of VS\n",
    "    part1 = (10**g)*( torch.pow(torch.norm(\n",
    "        torch.matmul(V,S)),2))\n",
    "    #Calculate the Frobenius norm of XTV\n",
    "    part2 = (10**l)*(torch.pow(torch.norm(\n",
    "        torch.matmul(torch.transpose(X,0,1),V)),2))\n",
    "    #Calculate the frobenius norm of V\n",
    "    part3 = (10**b)*(torch.pow(torch.norm(V),2))\n",
    "    return part1 + part2 + part3\n",
    "\n",
    "\n",
    "\n",
    "#Test values, just getting it to work\n",
    "iter = 50\n",
    "optimizer = optim.Adam([VIter],lr =0.01)\n",
    "\n",
    "#Simple loop for training\n",
    "for i in range(iter):\n",
    "\n",
    "    if VIter.grad is not None:\n",
    "        VIter.grad.data.zero_()\n",
    "    \n",
    "    y_pred = forward(X,VIter,S)\n",
    "    #These values were derived by trial and error for hyperparameters\n",
    "    #Should create a script to vary, and pick optimal\n",
    "    loss = loss_func(y_pred,Y) + reg(VIter,X,S,-6,-6,-6) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0022,  0.0243,  0.0035,  ..., -0.0102, -0.0059,  0.0078],\n",
      "        [ 0.0092,  0.0012, -0.0014,  ..., -0.0009, -0.0002, -0.0163],\n",
      "        [-0.0017, -0.0052, -0.0078,  ...,  0.0012, -0.0005, -0.0084],\n",
      "        ...,\n",
      "        [ 0.0155,  0.0060,  0.0006,  ...,  0.0027, -0.0020,  0.0187],\n",
      "        [-0.0065, -0.0038,  0.0019,  ...,  0.0011, -0.0008, -0.0045],\n",
      "        [-0.0103, -0.0056, -0.0014,  ..., -0.0102, -0.0059, -0.0112]],\n",
      "       dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Used this to check if it diverged to nan or not\n",
    "#print(VIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iterative accuracy is: 57.0401978231754\n",
      "The closed form accuracy is: 46.75198175168951\n"
     ]
    }
   ],
   "source": [
    "#Inference stage\n",
    "outPut = torch.tensor([])\n",
    "\n",
    "#Label test data\n",
    "xPrime = torch.tensor(test_vec)\n",
    "sPrime = torch.tensor(test_sig)\n",
    "\n",
    "#calculates the outputs\n",
    "outputs_1 = torch.matmul(torch.matmul(torch.transpose(xPrime,0,1),VIter),sPrime)\n",
    "#calculating the predictions\n",
    "preds_1 = torch.tensor([torch.argmax(output) for output in outputs_1])\t\t\n",
    "\n",
    "#creating confusion matrix\n",
    "cm = confusion_matrix(labels_test, preds_1)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "avg = sum(cm.diagonal())/len(test_labels_unseen)\n",
    "\n",
    "print(\"The iterative accuracy is:\", avg * 100)\n",
    "\n",
    "#trainval set\n",
    "d_trainval = trainval_vec.shape[0]\n",
    "a_trainval = trainval_sig.shape[0]\n",
    "W = np.zeros((d_trainval,a_trainval))\n",
    "gamm1 = 3\n",
    "alph1 = 0\n",
    "\n",
    "part_1_test = np.linalg.pinv(np.matmul(trainval_vec, trainval_vec.transpose()) + (10**alph1)*np.eye(d_trainval))\n",
    "part_0_test = np.matmul(np.matmul(trainval_vec,gt_trainval),trainval_sig.transpose())\n",
    "part_2_test = np.linalg.pinv(np.matmul(trainval_sig, trainval_sig.transpose()) + (10**gamm1)*np.eye(a_trainval))\n",
    "\n",
    "W = np.matmul(np.matmul(part_1_test,part_0_test),part_2_test)\n",
    "W = torch.tensor(W)\n",
    "#calculates the outputs\n",
    "outputs_1 = torch.matmul(torch.matmul(torch.transpose(xPrime,0,1),W),sPrime)\n",
    "#calculating the predictions\n",
    "preds_1 = torch.tensor([torch.argmax(output) for output in outputs_1])\t\t\n",
    "\n",
    "#creating confusion matrix\n",
    "cm = confusion_matrix(labels_test, preds_1)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "avg = sum(cm.diagonal())/len(test_labels_unseen)\n",
    "\n",
    "print(\"The closed form accuracy is:\", avg * 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
